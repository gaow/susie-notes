\section{\susie model}

To recap,
\begin{align}
\vy &= \mx\vb + \ve \\
\ve & \sim N(0,\rv I_n) \\ 
\vb &= \sum_{l=1}^L \vb_l\\
\vb_l & = \vgamma_l b_l \qquad (\text{independently for } l=1,\dots,L)\\
\vgamma_l & \sim \text{Mult}(1,\vpi) \\
b_l & \sim N(0,\rv_{0l}).
\end{align}
We use a mean-field approximation to posterior,
\begin{align}
q(\vb_1, \ldots, \vb_L) = \prod_l q_l(\vb_l)
\end{align}

\section{\susie VEM updates}

This section derives equations (A.29) -- (A.36) in the manuscript. 

\subsection{Bayesian univariate regression} \label{sec:bur}

This corresponds to equations (A.4) -- (A.8) in the manuscript. By Bayes rule,
\begin{equation}
    \post(b|\vy, \rv, \rv_0) = \frac{p(\vy|\rv,b)p(b|\rv_0)}{p(\vy|\rv, \rv_0)}
\end{equation}
where the prior
\begin{equation}
    \log p(b|\rv_0) = -\frac{1}{2}\log (2\pi\rv_0) - \frac{1}{2\rv_0}b^2
\end{equation}
and log-likelihood
\begin{equation}
\log p(\vy|\rv, b) = -\frac{n}{2}\log(2\pi\rv) - \frac{1}{2\rv}\norm{\vy - \vx b}^2
\end{equation}
Given $\rv$ and $\rv_0$ the marginal log-likelihood is:
\begin{align}
    \log p(\vy|\rv, \rv_0) &= \log \int p(\vy|\rv, b) p(b|\rv_0)\dif b \label{eqn:bur_marginal}\\
    & = \int \big(-\frac{n}{2} \log(2\pi\rv) - \frac{1}{2\rv}\norm{\vy-\vx b}^2 - \frac{1}{2}\log(2\pi) - \frac{1}{2}\log\rv_0 - \frac{1}{2\rv_0}\norm{b}^2\big)\dif b \\
    & = -\frac{n}{2}\log (2\pi\rv) - \frac{1}{2}\log(2\pi) - \frac{1}{2}\log \rv_0 \\
    & -\frac{1}{2} \int \big( \frac{\vy^T\vy - 2b\vx^T\vy + \vx^T b^2\vx}{\rv} + \frac{b^2}{\rv_0}\big) \dif b \label{eqn:bur_kernel}
\end{align}
Let $\tau_n = \vx^T\vx + \frac{\rv}{\rv_0}$, $\mu_1 = \vx^T\vy / \tau_n = \vx^T\vx \hat{b} / \tau_n$ where $\hat{b}$ is OLS estimate of regression coefficient. Then
\begin{align}
    \eqref{eqn:bur_kernel} &= -\frac{1}{2}\int (\tau_nb^2 - 2\tau_n \mu_1 b + \vy^T\vy) / \rv \dif b \\ 
    &= -\frac{1}{2}\int \big[ \tau_n(b^2 - 2\mu_1 b + \mu_1^2) / \rv + (\vy^T\vy - \tau_n\mu_1^2) / \rv \big] \dif b \\ 
    &= \frac{1}{2} \int \big[\norm{b-\mu_1}^2 / (\rv/\tau_n) + (\vy^T\vy - \tau_n\mu_1^2) / \rv \big] \dif b
\end{align}
Let $\rv_1 = \rv / \tau_n$, 
\begin{align}
    \eqref{eqn:bur_marginal} &= -\frac{n}{2}\log(2\pi\rv) - \frac{1}{2} \log \rv_0 \\
    &+ \int \big[-\frac{1}{2} \log(2\pi\rv_1) - \frac{1}{2\rv_1}\norm{b-\mu_1}^2 \big] \dif b \\
    &+ \frac{1}{2}\log \rv_1 - \frac{\vy^T\vy}{2\rv} + \frac{\mu_1^2}{2\rv_1} \\
    &= -\frac{n}{2}\log(2\pi\rv) - \frac{1}{2} \log \rv_0 + \frac{1}{2}\log \rv_1 - \frac{\vy^T\vy}{2\rv} + \frac{\mu_1^2}{2\rv_1} \label{eqn:bur_marginal_result}
\end{align}
Posterior of $b$ is given by
\begin{equation}
b | \vy, \rv, \rv_0 \sim N(\mu_1, \rv_1)
\end{equation}
The log Bayes factor is
\begin{align}
    \log \BF & := \log p(\vy|\rv, \rv_0) - \log p(\vy | \rv) \\
    &= \eqref{eqn:bur_marginal_result} - (-\frac{n}{2}\log(2\pi\rv) - \frac{\vy^T\vy}{2\rv}) \\
    &= - \frac{1}{2} \log \rv_0 + \frac{1}{2}\log \rv_1 + \frac{\mu_1^2}{2\rv_1}
\end{align}
Thus 
\begin{align}
\BF(\vy, \vx; \rv, \rv_0) &= (\rv_0/\rv_1)^{-\frac{1}{2}}\exp(\frac{\mu_1^2}{2\rv_1}) \\
& = \sqrt{\rv_1/\rv_0}\exp{\big(\frac{1}{2}z^2\frac{\rv_1}{\rv/\vx'\vx}\big)} 
\end{align}
where $z:=\frac{\hat{b}(\vy, \vx)}{\rv/\sqrt{\vx'\vx}}$.

\subsection{Bayesian single effect multiple regression} \label{sec:bser}

Here we derive equation (A.11). For each single effect $j$ the posterior inclusion probability is
\begin{align}
    \valpha_j &= p(\gamma_j = 1 | \vy, \rv, \rv_0) \\
    &= \frac{p(\vy | \gamma_j = 1, \rv, \rv_0)p(\gamma_j=1)}{\sum_j p(\vy | \gamma_j = 1, \rv, \rv_0)p(\gamma_j=1)} \\
    &= \pi_j \frac{p(\vy | \gamma_j = 1, \rv, \rv_0) / p(\vy|\rv)}{\sum_j \pi_j p(\vy | \gamma_j = 1, \rv, \rv_0) / p(\vy | \rv)} \\
    &= \pi_j \frac{\BF(\vy, \vx_j; \rv, \rv_0)}{\sum_j \pi_j \BF(\vy, \vx_j; \rv, \rv_0)} \label{eqn:ser alpha}
\end{align}
Derivations of single effect Bayes factors and posteriors follow directly from Section \ref{sec:bur}. We have now completed derivation of equations (A.11) -- (A.13) of the manuscript, the core updates of the \susie VEM algorithm.

\section{ELBO updates}

This section shows derivation of equation (B.19) for \susie model. It involves the ERSS in (B.22) and the marginal likelihood in (B.17) for Bayesian single effect model in Section \ref{sec:bser}: 

\begin{align}
    F(\q,\g,\rv; \vy) &= -\frac{n}{2} \log(2\pi\rv) -\frac{1}{2\rv}\E_{\q}\norm{\vy-\sum_l \vmu_l}^2 + \sum_l \E_{q_l}\left[\log\frac{g_l(\vmu_l)}{q_l(\vmu_l)} \right] \label{eqn:Fexplicit}\\
    &= -\frac{n}{2} \log(2\pi\rv) - \frac{1}{2\rv} \ERSS(\vy,\emu,\emusq) + \sum_l \E_{q_l}\left[ \log\frac{g_l(\vmu_l)}{q_l(\vmu_l)} \right]  
\end{align}

\subsection{ERSS}

For \susie model $\vmu_l = \mx \vb_l$. Let $\bb_{l_j} = \E_{q_l}[b_{l_j}]$, $\bbb_{l_j} = \E_{q_l}[b^2_{l_j}]$, and $\vbb = \sum_l \vbb_{l}$. The expectations are given by (A.14) and (A.15) in the manuscript. ERSS in (B.22) can be written as:
\begin{align}
    \text{(B.22)} &= \ERSS(\vy,\emu,\emusq) = \norm{\vy-\sum_l \emul}^2 - \sum_l \bar{\vmu_l}^T \bar{\vmu_l} + \sum_l \sum_i \E_{q_l}[(\mu_{l_i})^2] \\
    &= \norm{\vy-\sum_l \mx \vbb_l}^2 - \sum_l (\mx \vbb_l)^T (\mx \vbb_l) + \sum_l \sum_i \sum_{j} \mx_{ij}^{2}  \bbb_{lj} \\
    &= \vy^T\vy - 2 \vy^T (\sum_l \mx \vbb_l) + (\sum_l \mx \vbb_l)^T(\sum_l \mx \vbb_l) - \sum_l (\mx \vbb_l)^T (\mx \vbb_l) + \sum_l \sum_i \sum_{j} \mx_{ij}^{2}  \bbb_{lj} \\
    &= \vy^T\vy - 2 \vy^T\mx \bar{\vb} + \bar{\vb}^T \mx^T\mx \bar{\vb} - \sum_l \bar{\vb}_l^T \mx^T \mx \bar{\vb}_l + \sum_l \sum_j (\mx^T \mx)_{jj} \bbb_{lj} \label{eqn:erss}
\end{align}

\subsection{Marginal log-likelihood for the $l$-th effect}

For single effect model,
\begin{align}
    \log L(\vy; \rv, \rv_0) &= \log \int p(\vy | \vb_l, \rv, \rv_0) p(\vb_l | \rv, \rv_0) \dif \vb_l\\
    &= \log \iint p(\vy | \vb_l, \vgamma, \rv, \rv_0) p(\vb_l | \vgamma, \rv_0) p(\vgamma | \vpi)  \dif \vb_l \dif \vgamma \\
    &= \log \int \sum_j p(\vy | b_{l_j}, \rv, \rv_0) p(b_{l_j}|\gamma_j = 1, \rv_0) p(\gamma_j = 1 | \vpi) \dif b_{l_j} \\
    &= \log \sum_j \pi_j \int N(\vy; \vx_jb_{l_j}, \rv I_n) N(b_{l_j}; 0, \rv_0) \dif b_{l_j} \\
    &= \log \sum_j \pi_j p(\vy|\vx_j, \rv, \rv_0) \\
    &= \log \sum_j \pi_j \big[\BF(\vy, \vx_j; \rv, \rv_0)p(\vy|\rv) \big]\\
    &= \log N(\vy; 0, \rv I_n) + \log \sum_j \pi_j \BF(\vy, \vx_j; \rv, \rv_0) \label{eqn:lmarginal}
\end{align}

\subsection{Conditional expected log-likelihood for the $l$-th effect}

For single effect model,
\begin{equation}
    \E_{q_l}[\log p(\vy|\vb_l,\rv)] = -\frac{n}{2}\log(2\pi\rv) - \frac{1}{2\rv}\E_{q_l}\norm{\vy - \mx\vb_l}^2  \label{eqn:lconditional}
\end{equation}
where
\begin{align}
    \E_{q_l}\norm{\vy - \mx\vb_l}^2 &= \norm{\vy - \mx \vbb_l}^2 - \vbb_l^T\mx^T\mx \vbb_l + \sum_i \sum_j \mx_{ij}^2 \bbb_{lj} \\
    &= \vy^T\vy -2 \vy^T \mx \vbb_l  + \sum_j (\mx^T \mx)_{jj} \bbb_{lj}
\end{align}

\subsection{Compute ELBO}
Now we are ready to compute ELBO (B.19).
Replacing $\vy$ by $\er$ in \eqref{eqn:lmarginal} and \eqref{eqn:lconditional}, we derive (B.34) in the manuscript:
\begin{align}
    \E_{\hat{q}_l} \left[ \log \frac{g_l(\vmu_l)}{\hat{q}_l(\vmu_l)} \right] &= \log L_l(\er; g_l,\rv) - \E_{q_l}[\log p(\er|\vmu_l,\rv)]  \\
    &= \log N(\er; 0, \rv I_n) + \log \sum_j \pi_j \BF(\er, \vx_j; \rv, \rv_0) + \frac{n}{2} \log(2\pi\rv) +\frac{1}{2\rv}\E_{\hat{q}_l}\norm{\er-\vmu_l}^2 \\
    &= -\frac{n}{2} \log(2\pi\rv) -\frac{1}{2\rv}\er^T\er + \log \sum_j \pi_j \BF(\er, \vx_j; \rv, \rv_0) + \\
    & \frac{n}{2} \log(2\pi\rv) +\frac{1}{2\rv}\left[\er^T\er - 2\er^T\mx \bar{\vb}_l + \sum_j (\mx^T \mx)_{jj} \bar{b}_{lj}^2 \right] \\
    &= \log \sum_j \pi_j \BF(\er, \vx_j; \rv, \rv_0) + \frac{1}{2\rv}\left[- 2\er^T\mx \bar{\vb}_l + \sum_j (\mx^T \mx)_{jj} \bar{b}_{lj}^2\right] \label{eqn:lkl}
\end{align}
The ELBO at $\q = \hat{\q}_{optim}$ is then given by $F(\hat{\q},\g,\rv; \vy) = -\frac{n}{2} \log(2\pi\rv) - \frac{1}{2\rv} \eqref{eqn:erss} + \sum_l \eqref{eqn:lkl}$.

\section{Update residual variance}
This is a crucial step but is trivial to derive for univariate case ($\vy$ is a vector). It is given by Equation (B.23) in the manuscript. Specifically for \susie model, $\hat{\rv} = \eqref{eqn:erss}/n$.

\section{Sufficient statistics for \susie}

The sufficient statistics required to fit \susie model are ($\mx^T\mx$, $\mx^T\vy$, $\vy^{T}\vy$ and $n$).

\subsection{VEM updates} \label{sec:vem_update_ss}
Using $\mx^T\mx$, $\mx^T\vy$, $\vy^{T}\vy$ and $n$ as input, results for (A.9) -- (A.15) in the manuscript still hold. For (B.27) the only missing part is $\mx^T\er$, which can be written as:
\begin{align}
\mx^T\er &= \mx^T[\vy - \sum_{l\neq l}\mx (\valpha_{l'} \circ \vmu_{l'})] \\
&= \mx^T\vy - \mx^T\mx\sum_{l'\neq l}(\valpha_{l'} \circ \vmu_{l'})
\end{align}
That is, VEM updates can be computed using $\mx^T\mx$, $\mx^T\vy$, $\vy^{T}\vy$ and $n$.

\subsection{ELBO computation} \label{sec:elbo_ss}
This boils down to computing \eqref{eqn:erss} and \eqref{eqn:lkl}. \eqref{eqn:erss} can be computed with VEM updates and $\mx^T\mx$, $\mx^T\vy$ and $\vy^{T}\vy$. Bayes factor in \eqref{eqn:lkl} is also given in VEM update; $\er^T\mx$ has been previously computed in Section \ref{sec:vem_update_ss}. It is thus possible to compute ELBO using $\mx^T\mx$, $\mx^T\vy$, $\vy^{T}\vy$ and $n$.

\section{\susie with summary statistics}

\subsection{Overview}
So far we have been working directly with column-centered data $\mx\in \mathbb{R}^{n\times p}$ and $\vy\in\mathbb{R}^{n\times 1}$. In practice, results from genetic association studies are often communicated and shared in terms of ``summary statistics''. Summary-level information in association analysis typically include:
\begin{enumerate}
\item Estimated effect size $\hat{b}_j$ and its standard error $\text{SE}(\hat{b}_j)$, from single-SNP analysis for each SNP $j$ (i.e., univariate analysis). 
\item Sample size of study, $n$.
\item Sample variance of the response, $\ry := \vy^T\vy/(n-1)$ (or, alternatively, $\vy^T\vy/n$). Note here $\vy$ is pre-centered. 
\item Correlation matrix between SNPs, $\mr=\text{cor}(\mx)$ (or some approximations to it). $\mr$ is commonly known as LD matrix (LD for Linkage Disequilibrium).
\end{enumerate}
To analyze summary statistics with \susie, we first verify that \susie VEM updates and ELBO computations depend in the data only through $\mx^T\mx$, $\mx^T\vy$, $n$ and $\ry$. We then replace these quantities and provide interface to \susie to work with either of the following summary statistics information:
\begin{enumerate}
    \item Sufficient summary statistics: $\hat{b}_j, \text{SE}(\hat{b}_j), \mr, n, \ry$
    \item Univariate testing $z$ scores: $z_j, \mr$
\end{enumerate}

One crucial difference between the two interfaces is the availability of sample size $n$. If the sample size is not available, we must use the univariate testing $z$ scores, which will loss some power. If the sample size is available, but the sample variance of response is unknown, we can use interface 1, sufficient summary statistics, with $\ry = 1$. The output coefficients are in standardized $\mx$, $\vy$ scale.

\subsection{\susie interface with sufficient summary statistics}
Let $\hat{s}_j = \text{SE}(\hat{b}_j)$. We assume the univariate analysis fits the model
\begin{equation}\label{eqn:SLR}
    \vy = b_{0j} + \vx_j b_j + \epsilon \quad \epsilon\sim N_n(\bm{0}, \sigma_j^2 I_n)
\end{equation}
Summary statistics are given by:
\begin{align}
    \hat{b}_{j} &= \frac{\vx_{j}^{T}\vy}{\vx_{j}^{T}\vx_{j}} \\
    \hat{s}_{j} &= \frac{\hat{\sigma}_{j}}{\sqrt{\vx_{j}^{T}\vx_{j}}} \label{eqn:shat}
\end{align}
where
\begin{equation}
    \hat{\sigma}_{j}^{2} = \frac{\norm{\vy - \hat{b}_{0j} - \vx_{j} \hat{b}_{j}}^2}{n-2} = \frac{RSS_{j}}{n-2}
\end{equation}
In addition to the summary data $(\hat{b}_j, \hat{s}_j)$, we assume that we have sample size n, the sample variance of response $\ry$, and the correlation matrix $\mr = cor(\mx)$.

\subsubsection{Method}

The z score $\hat{z}_j$ is
\begin{equation}
     \hat{z}_{j} = \frac{\hat{b}_{j}}{\hat{s}_{j}} = \frac{\vx_{j}^{T}\vy}{\hat{\sigma}_{j}\sqrt{\vx_{j}^{T}\vx_{j}}}
\end{equation}
\begin{proposition} \label{prop:Cor Coeff}
Correlation coefficient $R^2$ for the corresponding simple linear regression model \eqref{eqn:SLR} is
\begin{equation}
    R_{j}^2 = \frac{\hat{z}_{j}^2}{\hat{z}_{j}^2+n-2},
\end{equation}
\end{proposition}

\begin{proof}
This follows directly from its definition involving the sum of squares due to regression (SSreg) and sum of residual error (RSS),
\begin{align}
    SSreg_j &= \vy^T \vy - \norm{\vy - \hat{b}_{0j} - \vx_{j} \hat{b}_{j}}^2 = \vy^T \vy - (\vy^T \vy - \frac{(\vx_{j}^{T}\vy)^2}{\vx_{j}^{T}\vx_{j}}) = \frac{(\vx_{j}^{T}\vy)^2}{\vx_{j}^{T}\vx_{j}}
\end{align}
\begin{align}
    R_{j}^2 &= \frac{SSreg_j}{SSreg_j + RSS_j} = \frac{\frac{(\vx_{j}^{T}\vy)^2}{\vx_{j}^{T}\vx_{j}}}{\frac{(\vx_{j}^{T}\vy)^2}{\vx_{j}^{T}\vx_{j}} + \hat{\sigma}_{j}^{2}(n-2)} \\
    &= \frac{\hat{z}_{j}^2}{\hat{z}_{j}^2+n-2}.
\end{align}
\end{proof}

\begin{proposition} \label{prop:resid var}
Using correlation coefficient $R^2_j$, the estimated residual variance for \eqref{eqn:SLR} is
\begin{equation}
    \hat{\sigma}_{j}^2 = \ry\frac{(n-1)(1-R_{j}^2)}{n-2} \label{eqn:sigma2hat}
\end{equation}
\end{proposition}
\begin{proof}
\begin{equation}
    1-R_{j}^{2} = \frac{RSS_j}{SSreg_j + RSS_j} = \frac{\hat{\sigma}_{j}^2(n-2)}{\ry(n-1)}
\end{equation}
\end{proof}
Using Proposition \ref{prop:Cor Coeff} and \ref{prop:resid var}, we have 
\begin{equation}\label{eqn:residj}
    \hat{\sigma}_{j}^2 = \ry \frac{n-1}{\hat{z}_{j}^2+n-2}
\end{equation}
We now work backwards to recovering $\mx^T\mx$ and $\mx^T\vy$:
\begin{align}
    \vx_j^{T}\vx_j &= \frac{\hat{\sigma}_{j}^2}{\hat{s}_{j}^{2}} \label{eqn:xtxjj} \\
    \vx_j^{T}\vy &= \frac{\hat{\sigma}_{j}^2}{\hat{s}_{j}} \hat{z}_{j} \label{eqn:xty}
\end{align}
Denote $\Lambda = \text{diag}(\sqrt{\vx_1^{T}\vx_1}, \cdots, \sqrt{\vx_p^{T}\vx_p})$, given $\mr = \text{cor}(\mx)$,
\begin{equation}\label{eqn:xtx}
    \mx^T \mx = \Lambda \mr \Lambda.
\end{equation}
We can now perform \susie computations along the lines of Sections \ref{sec:vem_update_ss} and \ref{sec:elbo_ss}.

\begin{algorithm}[H] 
\caption{Transform sufficient summary statistics to $\mx^T \mx$, $\mx^T \vy$ (outline)} \label{alg:ss suff interface}
\begin{algorithmic}[1]
\Require $\hat{\vb}$, $\hat{\bm{s}}$, $\mr$, $\ry$, n.
\State Compute $\hat{z}_j := \hat{b}_{j} / \hat{s}_j $
\State Compute $\hat{\sigma}_{j}^2$ = \eqref{eqn:residj}
\State Compute $\vx_j^{T}\vx_j = $ \eqref{eqn:xtxjj}, $\Lambda = \text{diag}(\sqrt{\vx_1^{T}\vx_1}, \cdots, \sqrt{\vx_p^{T}\vx_p})$
\State $\mx^T \mx = \Lambda \mr \Lambda$
\State $\vx_j^{T}\vy = $ \eqref{eqn:xty} \\
\Return $\mx^T \mx$, $\mx^{T}\vy$, $\ry$, n
\end{algorithmic}
\end{algorithm}

\subsubsection{Single Effect Regression Algorithm}
The Single Effect Regression model is:
\begin{align} \label{eqn:SER}
\vy &= \mx\vb + \ve \\ \label{eqn:SER-e}
\ve & \sim N(0,\rv I_n) \\  \label{eqn:SER-b0}
\vb & = \vgamma b \\
\vgamma & \sim \text{Mult}(1,\vpi) \\ \label{eqn:SER-b}
b & \sim N(0,\rv_0),
\end{align}
The likelihood function is
\begin{equation} \label{eqn:ser L}
    L(\vy; \rv_0, \rv) := p(\vy |\rv_0, \rv) = \sum_{j=1}^{p} \pi_j N(\hat{b}_{j};0, \frac{\rv}{\vx_j^{T}\vx_j} + \rv_0 )
\end{equation}
For each single effect $j$ the posterior inclusion probability is $\alpha_j= \eqref{eqn:ser alpha}$.
\begin{align}
    \BF(\vy, \vx_j; \rv, \rv_0) &= \sqrt{\frac{\frac{\rv}{\vx_{j}^{T}\vx_j}}{\frac{\rv}{\vx_{j}^{T}\vx_j} + \sigma^2_0}} \exp{\left(\frac{1}{2}\frac{\vx_{j}^{T}\vx_{j} \hat{b}_j^2}{\hat{\sigma}_{j}^2} \frac{\hat{\sigma}_{j}^2}{\rv} \frac{\sigma^2_0}{\frac{\rv}{\vx_{j}^{T}\vx_j} + \sigma^2_0}\right)} \\
    &= \sqrt{\frac{\frac{\rv}{\vx_{j}^{T}\vx_j}}{\frac{\rv}{\vx_{j}^{T}\vx_j} + \sigma^2_0}} \exp{\left(\frac{1}{2} \hat{z}_{j}^2 \frac{\hat{\sigma}_{j}^2}{\rv} \frac{\sigma^2_0}{\frac{\rv}{\vx_{j}^{T}\vx_j} + \sigma^2_0}\right)} \label{eqn:SERBF}
\end{align}
The posterior distribution for $b$ is
\begin{align}
b_j | \vy, \rv, \rv_0, \gamma_j=1 &\sim N(\mu_{1j},\rv_{1j})
\intertext{ where }
\rv_{1j}(\vx_j) & = (\frac{\vx_j^{T}\vx_j}{\rv} + \frac{1}{\sigma^{2}_0})^{-1} = (\frac{\ry(n-1)}{\rv\hat{s}_j^{2}(\hat{z}_j^2+n-2)} + \frac{1}{\sigma^{2}_0})^{-1}\label{eqn:tau1} \\ 
\mu_{1j}(\vy,\vx_j) & = \frac{\vx_j^{T}\vx_j\sigma_{1j}^2}{\rv} \hat{b}(\vy,\vx_j) = \frac{\ry(n-1)\rv \hat{z}_j}{\ry(n-1) + \frac{\rv}{\sigma^2_0}\hat{s}_j^2(\hat{z}_j^2+n-2)}, \label{eqn:mu1}
\end{align}
The second moment of $b_j | \vy, \rv, \rv_0, \gamma_j=1$ is \begin{equation}\label{eqn:mu12}
    \widebar{\mu^2}_{1j} = \rv_{1j} + \mu_{1j}^2
\end{equation}
The first moment and the second moment of $b_j$ is 
\begin{align}
    \bb_j &= \mathbb{E}(b_j|\rv, \rv_0) = \alpha_j \mu_{1j} \label{eqn:ser 1st}\\
    \bbb_j &= \mathbb{E}(b_j^2|\rv, \rv_0) = \alpha_j (\rv_{1j} + \mu_{1j}^2) = \alpha_j \widebar{\mu^2}_{1j} \label{eqn:ser 2nd}
\end{align}
Let $\hat{q}(\vb)$ be the posterior distribution of $\vb$, the ELBO is
\begin{align}
    F(\hat{\q},\g,\rv; \vy) &= -\frac{n}{2} \log(2\pi\rv) - \frac{1}{2\rv}(\ry (n-1) - 2 \vy^T\mx \bar{\vb} + \sum_j (\mx^T \mx)_{jj} \bar{b^2}_{j}) + \E_{\hat{q}}\left[\log\frac{g(\vb)}{\hat{q}(\vb)} \right] \label{eqn:ser elbo}
\end{align}
Maximizing \eqref{eqn:ser elbo} over $\rv$, we have
\begin{equation}\label{eqn:ser resid}
    \hat{\rv} = \frac{\ry (n-1) - 2 \vy^T\mx \bar{\vb} + \sum_j (\mx^T \mx)_{jj} \bbb_{j}}{n}
\end{equation}

\begin{algorithm}[H] 
\caption{SER using sufficient statistics (outline)} \label{alg:SERalg}
\begin{algorithmic}[1]
\Require $\mx^T \mx$, $\mx^{T}\vy$, $\ry$, n.
\Repeat
\State $\hat{\rv}_0 \gets \arg \max$ \eqref{eqn:ser L} \Comment{EB update of $\rv_0$ (optional; omitted if $\rv_0$ fixed)} 
\State $\alpha_j, \mu_{j1}, \widebar{\mu^2}_{1j} \gets $ \eqref{eqn:ser alpha} \eqref{eqn:mu1} \eqref{eqn:mu12} \Comment{update posterior}
\State $\bb_j$, $\bbb_j$ $\gets$ \eqref{eqn:ser 1st}, \eqref{eqn:ser 2nd} \Comment{update posterior}
\State $\hat{\rv} \gets $ \eqref{eqn:ser resid} \Comment{update $\rv$ (optional; omitted if $\rv_0$ fixed)}
\State Update ELBO \eqref{eqn:ser elbo}
\Until{converged} \\
\Return $\hat{\rv}_0$, $\vmu$, $\emusq$, $\valpha$, $\hat{\rv}$
\end{algorithmic}
\end{algorithm}

\subsubsection{\susie algorithm}
The \susie model is a sum of single effect regression. We summarize the algorithm here:
\begin{algorithm}[H] 
\caption{\susie using sufficient summary statistics (outline)} \label{alg:susiealg}
\begin{algorithmic}[1]
\Require $\hat{\vb}$, $\hat{\bm{s}}$, $\mr$, $\ry$, n.
\State Compute $\mx^T \mx$, $\mx^{T}\vy \gets $ Algorithm \ref{alg:ss suff interface} 
\Repeat
\For{$l$ in $1,\dots,L$}
	\State $\mx^{T}\vr_l \gets \mx^{T}\vy - \sum_{l'\neq l} \mx^{T}\mx \bar{\vb}_{l'}$ \Comment{compute residuals}
    \State ($\hat{\rv}_{l0}$, $\vmu_l$, $\emusq_l$, $\valpha_l$) $\gets \SER(\mx^T\mx, \mx^T\vr_l,\ry, n)$ \Comment{SER to residuals, omit $\rv$}
    \State $\bar{\vb}_l \gets \valpha_l \circ \vmu_l$ \Comment{compute posterior mean; $\circ$ is element-wise multiplication}
    \State $\bar{\vb^2}_l \gets \valpha_l \circ \emusq_l$ \Comment{compute posterior second moment}
\EndFor
\State $\bar{\vb} \gets \sum_l \bar{\vb}_l$
\State $\hat{\rv} \gets$ \eqref{eqn:erss}/n
\State Update ELBO \eqref{eqn:Fexplicit}
\Until{converged} \\
\Return $\hat{\vrv}_0$, $\vmu_{L\times p}$, $\emusq_{L\times p}$, $\valpha_{L\times p}$, $\hat{\rv}$
\end{algorithmic}
\end{algorithm}

\subsection{\susie using univariate testing $z$ scores} \label{sec:summary z}
\subsubsection{Bayesian simple linear regression}
We can rewrite the equations (A.1) - (A.3) as
\begin{align}
\vy &= \sigma\left( \frac{1}{\sqrt{n-1}} \vx \frac{b\sqrt{n-1}}{\sigma} + \frac { 1}{\sigma} \ve \right) \\
 &= \sigma\left( \frac{1}{\sqrt{n-1}} \vx \tilde{b} + \tilde{\ve} \right) \label{eqn:BSLRz-y}\\
\tilde{b} &\sim N(0, \frac{(n-1) \rv_{0}}{\rv}) \\
\tilde{\ve} & \sim N(0,I_n) \label{eqn:BSLRz-e}
\end{align}
Define z score as 
\begin{equation}
    z(\sigma) \equiv \frac{\frac{1}{\sqrt{n-1}}\vx^T \frac{1}{\sigma}\vy}{ \sqrt{\frac{1}{n-1}\vx^T\vx}},
\end{equation}
which is scale-invariant of $\vx$. For simplicity, we assume $\vx$ is standardized, $\frac{1}{n-1}\vx^T\vx = 1$. The z score can be written as
\begin{equation}\label{eqn:z}
    z(\sigma) = \frac{1}{\sqrt{n-1}}\vx^T \frac{1}{\sigma}\vy
\end{equation}
Plug in \eqref{eqn:BSLRz-y},
\begin{align}
    \eqref{eqn:z} &= \frac{1}{\sqrt{n-1}}\vx^T \left( \frac{1}{\sqrt{n-1}} \vx \tilde{b} + \tilde{\ve} \right) \label{eqn:BSLRz-z}\\
    &= \tilde{b} + \frac{1}{\sqrt{n-1}}\vx^T\tilde{\ve} \\
    z(\sigma) &\sim N(\tilde{b}, 1) \quad \text{because of \eqref{eqn:BSLRz-e}} \label{eqn:BSLRz-zdistr}
\end{align}
We use $w^2$ to denote $(n-1)\frac{\rv_0}{\rv}$. The posterior distribution for $\tilde{b}$ is
\begin{align}
    \tilde{b} | z(\sigma), w &\sim N(\mu_1, \rv_1) \\
    \rv_1 &= (1 + w^{-2})^{-1} \\
    \mu_1 &= \rv_1 z(\sigma)
\end{align}
The Bayes Factor is 
\begin{align}
    \BF(z(\sigma); w) &=  \frac{p(z(\sigma) | \tilde{b} \neq 0)}{p(z(\sigma) | \tilde{b} = 0)}\label{eqn:BSLRz-BF} \\
    &= \frac{N(z(\sigma); 0, 1+w^2)}{N(z(\sigma); 0, 1)} \\
    &= \sqrt{\frac{1}{1+w^2}} \exp\left( \frac{1}{2} z(\sigma)^2 \frac{w^2}{1+w^2} \right)
\end{align}
In practice, $\rv$ is not known, \cite{wen2014bayesian} shows the Bayes Factor for any $\rv$ can be approximated by Laplace's method.
\begin{equation}
    \BF(z(\sigma); w) =  \BF(z(\hat{\sigma}); w) \left[1+o(\frac{1}{n})\right]
\end{equation}
where $\hat{\rv}$ is the MLE of $\rv$ estimated from the simple linear regression model testing the association of $\vx$. We also use $\hat{\rv}$ in the posterior mean. Note that $z(\hat{\sigma})$ is the z score from simple linear regression.

\subsubsection{Single Effect Regression}
The single effect regression model is
\begin{align}
\vy &= \sigma\left( \frac{1}{\sqrt{n-1}} \mx \frac{\vb\sqrt{n-1}}{\sigma} + \frac {1}{\sigma} \ve \right) \\
 &= \sigma\left( \frac{1}{\sqrt{n-1}} \mx \tilde{\vb} + \tilde{\ve} \right) \label{eqn:SERz-y}\\
\tilde{\vb} &= \tilde{b} \vgamma \\
\vgamma &\sim Mult(1, \vpi) \\
\tilde{b} &\sim N(0, w^2) \\
\tilde{\ve} & \sim N(0,I_n) \label{eqn:SERz-e}
\end{align}
We assume that $\mx$ is column standardized. The z score for each SNP j is
\begin{equation}\label{eqn:zj}
    z_j(\sigma) = \frac{1}{\sqrt{n-1}}\vx_j^T \frac{1}{\sigma}\vy
\end{equation}
Similar as \eqref{eqn:BSLRz-z} - \eqref{eqn:BSLRz-zdistr}, we have
\begin{equation} \label{eqn:serz}
    \bm{z}(\sigma) \sim N(\mr \tilde{\vb}, \mr)
\end{equation}
where $\mr$ is the sample correlation matrix (LD matrix). The posterior distribution for $\tilde{\vb}$ is
\begin{align} \label{eqn:serz-postgamma}
    \vgamma | \bm{z}(\sigma), w &\sim Mult(1, \valpha) \\
    \alpha_j &= \BF(z_j(\sigma); w) \pi_j/\sum_k \BF(z_k(\sigma); w) \pi_k \label{eqn:serz-postalpha}\\
    \tilde{b} | z(\sigma), w, \vgamma_j = 1 &\sim N(\mu_{1j}, \rv_{1j}) \label{eqn:serz-postb}\\
    \rv_{1j} &= (1 + w^{-2})^{-1} \label{eqn:serz-postsigma}\\
    \mu_{1j} &= \rv_{1j} z_j(\sigma) \label{eqn:serz-postmu}
\end{align}
For each SNP j, the Bayes Factor is
\begin{align} \label{eqn:SERzBFj}
    BF(z_j(\sigma); w) &= \sqrt{\frac{1}{1+w^2}} \exp\left( \frac{1}{2} z_j(\sigma)^2 \frac{w^2}{1+w^2} \right)
\end{align}
The first moment and the second moment of $\tilde{b}_j$ is 
\begin{align}
    \bb_j &= \mathbb{E}(\tilde{b}_j|w) = \alpha_j \mu_{1j} \label{eqn:serz 1st}\\
    \bbb_j &= \mathbb{E}(\tilde{b}_j^2|w) = \alpha_j (\rv_{1j} + \mu_{1j}^2) = \alpha_j \widebar{\mu^2}_{1j} \label{eqn:serz 2nd}
\end{align}
Let $\hat{q}(\tilde{\vb})$ be the posterior distribution of $\vb$, the ELBO is
\begin{align}
    F(\hat{\q},\g;\bm{z}) &= -\frac{n}{2} \log(2\pi) - \frac{1}{2}(\frac{1}{\rv}\vy^T\vy - 2 \frac{1}{\sigma}\vy^T\frac{1}{\sqrt{n-1}}\mx \bar{\vb} + \sum_j \bbb_{j}) + \E_{\hat{q}}\left[\log\frac{g(\tilde{\vb})}{\hat{q}(\tilde{\vb})} \right] \\
    &= \bm{z}(\sigma)^{T} \bar{\vb} - \frac{1}{2} \sum_j \bbb_{j} + \E_{\hat{q}}\left[\log\frac{g(\tilde{\vb})}{\hat{q}(\tilde{\vb})}\right] + C\label{eqn:SERzelbo}
\end{align}
where $C$ is a constant.

Similar as in Bayesian simple linear regression, we use $\hat{\rv}_j$, which is the MLE of $\rv_j$ estimated from the simple linear regression model testing the association of SNP j, to approximate the Bayes Factor and the posterior distribution. The estimates $z_j(\hat{\sigma}_j)$ is the z score from the simple linear regression model for each SNP j.

\begin{algorithm}[H] 
\caption{SER using $z$ scores} \label{alg:SERzalg}
\begin{algorithmic}[1]
\Require $\bm{z}$, $\mr$
\State $\hat{w} \gets \arg \max \sum_{j=1}^{p} \pi_j BF(z_j(\hat{\sigma}_j); w)$ \Comment{EB update of $w$} 
\State $\alpha_j, \mu_{1j}, \widebar{\mu^2}_{1j} \gets $ \eqref{eqn:serz-postalpha} - \eqref{eqn:serz-postmu} \Comment{update posterior} \\
\Return $\hat{w}$, $\vmu$, $\emusq$, $\valpha$
\end{algorithmic}
\end{algorithm}

\textbf{Connection with RSS:}

We derived the Bayes Factor \eqref{eqn:SERzBFj} using the RSS likelihood \citep{zhu2017bayesian}. The RSS likelihood assumes the correlation of $\vy$ with any single covariate $\vx_j$ is small. Let $\hat{\vb} = (\hat{b}_1, \cdots, \hat{b}_p)^T$, $\hmd = diag(\hvd)$, $\hvd = (\hat{d}_1, \cdots, \hat{d}_p)^T$, 
\begin{equation}
    \hat{d}_j^2 = \hat{s}_j^2 + \frac{1}{n} \hat{b}_j^2 \approx \hat{s}_j^2 \text{ when n is large}
\end{equation}
\cite{zhu2017bayesian} show the likelihood for $\vb$ is
\begin{equation}
    L_{RSS}(\vb; \hat{\vb}, \hmd, \mr) = N(\hat{\vb}; \hmd \mr \hmd^{-1}\vb, \hmd \mr \hmd)
\end{equation}
Let $\vz = \hmd^{-1}\vb$, $\hvz = \hmd^{-1}\hat{\vb}$, then
\begin{equation}
    \hvz | \vz, \mr \sim N(\mr \vz, \mr)
\end{equation}
which is very similar as \eqref{eqn:serz}.
\begin{equation}
    \left(\begin{array}{c}
        \hat{\vz}_{-j}  \\
        \hat{z}_j 
    \end{array}\right) | \vz, \mr \sim N\left(\left(\begin{array}{c}
        (\mr\vz)_{-j}  \\
        (\mr\vz)_{j}
    \end{array}\right), \left(\begin{array}{cc}
        \mr_{-jj} & (\mr_{j})_{-j} \\
        (\mr_{j})_{-j}^{T} & 1
    \end{array} \right) \right)
\end{equation}
where $\mr_{-jj}$ is the $\mr$ matrix without the $j$th row and column. $(\mr_{j})_{-j}$ is the $j$th column of $\mr$ without the $j$th element. Using conditional probability, we have
\begin{align}
    p(\hvz|z_j=0, \vz = 0) &= p(\hat{z}_j | z_j=0, \vz = 0) p(\hat{\vz}_{-j}| \hat{z}_j,  z_j=0, \vz = 0) \\
    &= N(\hat{z}_j; 0,1)N(\hat{\vz}_{-j}; (\mr_{j})_{-j} \hat{z}_j, \mr_{-jj} - (\mr_{j})_{-j} (\mr_{j})_{-j}^{T})
\end{align}
\begin{align}
    p(\hvz|z_j\neq0, \vz_{-j} = 0) &= p(\hat{z}_j | z_j\neq0, \vz_{-j} = 0) p(\hat{\vz}_{-j}| \hat{z}_j,  z_j\neq0, \vz_{-j} = 0) \\
    &= N(\hat{z}_j; z_j,1)N(\hat{\vz}_{-j}; (\mr_{j})_{-j} \hat{z}_j, \mr_{-jj} - (\mr_{j})_{-j} (\mr_{j})_{-j}^{T})
\end{align}
The Bayes Factor is 
\begin{align}
    BF &= \frac{\int p(\hvz|z_j\neq0, \vz_{-j} = 0)p(z_j) d z_j }{p(\hvz|z_j=0, \vz = 0)} \\
    &= \frac{\int N(\hat{z}_j; z_j,1)N(z_j;0,w)d z_j}{N(\hat{z}_j; 0,1)} \\
    &= \frac{N(\hat{z}_j; 0,1+w^2)}{N(\hat{z}_j; 0,1)} \\
    &= \sqrt{\frac{1}{1+w^2}} \exp{\left[ \frac{1}{2} \hat{z}_j^2 (1+\frac{1}{w^2})^{-1} \right]}
\end{align}
which is same as \eqref{eqn:SERzBFj} with $z_j(\hat{\sigma}_j)$.

\subsubsection{\susie algorithm}
The \susie model is a sum of single effect regression. The ELBO at optimum $\q$ is
\begin{align}
    F(\hat{\q},\g;\bm{z}(\sigma)) &= \bm{z}(\sigma)^{T} \bar{\vb} - \frac{1}{2}\bar{\vb}^T \mr \bar{\vb} + \frac{1}{2} \sum_l \bar{\vb}_l^T \mr \bar{\vb}_l - \frac{1}{2} \sum_l \sum_j \bar{b^2}_{lj} + \sum_l \E_{\hat{q}_l}\left[\log\frac{g_l(\vmu_l)}{\hat{q}_l(\vmu_l)} \right] \label{eqn:Fzexplicit}
\end{align}
We summarize the algorithm here:
\begin{algorithm}[H] 
\caption{\susie using univariate testing $z$ scores (outline)} \label{alg:susiezalg}
\begin{algorithmic}[1]
\Require $\bm{z}$, $\mr$.
\Repeat
\For{$l$ in $1,\dots,L$}
	\State $\bm{z}_{\vr l} \gets \bm{z} - \sum_{l'\neq l} \mr \bar{\vb}_{l'}$ \Comment{compute residuals}
    \State ($\hat{w}_{l}$, $\vmu_l$, $\emusq_l$, $\valpha_l$) $\gets \SER(\bm{z}_{\vr}, \mr)$ \Comment{SER to residuals}
    \State $\bar{\vb}_l \gets \valpha_l \circ \vmu_l$ \Comment{compute posterior mean; $\circ$ is element-wise multiplication}
    \State $\widebar{\vb^2}_l \gets \valpha_l \circ \emusq_l$ \Comment{compute posterior second moment}
\EndFor
\State $\bar{\vb} \gets \sum_l \bar{\vb}_l$
\State Update ELBO \eqref{eqn:Fzexplicit}
\Until{converged} \\
\Return $\hat{\bm{w}}$, $\vmu_{L\times p}$, $\emusq_{L\times p}$, $\valpha_{L\times p}$
\end{algorithmic}
\end{algorithm}
Note that when the data contain multiple strong association signals ($\hat{\sigma}_j > \sigma$), the z score approximation over-estimates residual error, therefore under-estimates the Bayes Factor. The over-estimation of noise can lead to reduce power.