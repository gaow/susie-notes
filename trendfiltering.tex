\section{\susie for trend filtering}
\subsection{Overview}

Trend filtering is a useful statistical tool for nonparametric regression. \cite{Kim07l1trend} first proposed $\ell_1$ trend filtering for estimating underlying piecewise linear trends in time series data. This idea can be further extended to fit piecewise polynomial of degree $k$ to the data. In their paper, Kim et al. showed the equivalence between the $\ell_1$ trend filtering and the $\ell_1$-regularized least squares problem. This motivates us to think about the connection between trend filtering and sparse approximation in general. 

\subsection{Trend filtering and sparse regression}
Trend filtering problem is defined mathematically as follows. For a given integer $k \geq 0$, the kth order trend filtering is defined by a penalized least squares optimization problem,
\begin{align}
\hat{\vb} = \underset{\vb}{\mathrm{argmin}} \frac{1}{2}|| \vy- \vb||_2^2 + \frac{n^k}{k!}\lambda||D^{(k+1)}\vb||_1,
\end{align}
where $\vy = [y_1 \dots y_n]^T$ is an n vector of observations, $\lambda$ is a tuning parameter, and $D^{(k+1)}$ is the discrete difference operator of order $k$. When order $k=0$, $D$ is defined 

\begin{align}
D^{(1)} = \begin{bmatrix} 
    -1 & 1 & 0 & \dots & 0 & 0\\
    0 & -1 & 1 & \dots & 0 & 0\\
    \vdots & \ddots & \\
    0 & 0 & 0 & \dots & -1 & 1\\
    \end{bmatrix}
    \in \mathbb{R}^{(n-1)\times n}.
\end{align}

In this case, the components of the trend filtering estimate form a piecewise constant structure, with break points corresponding to the nonzero entries of $D^{(1)}\hat{\vb} = (\hat{b}_2 - \hat{b}_1, \dots, \hat{b}_n - \hat{b}_{n-1})$ \cite{Tibshirani2014}. And when $k\geq 1$, the operator $D^{(k+1)}$ is defined recursively,

\begin{align}
D^{(k+1)} = D^{(1)} \cdot D^{(k)} \in \mathbb{R}^{(n-k-1)\times n}.
\end{align}
Now we want to transform the trend filtering problem into a sparse regression problem. Let $\bm{\beta}=D^{(k+1)}\vb$. Then if $D^{(k+1)}$ were invertibe, we could write $\vb = (D^{(k+1)})^{-1}\bm{\beta}$ and the above problem would become

\begin{align}
\hat{\bm{\beta}} = \underset{\bm{\beta}}{\mathrm{argmin}} \frac{1}{2}||\vy- (D^{(k+1)})^{-1}\bm{\beta}||_2^2 + \frac{n^k}{k!}\lambda||\bm{\beta}||_1.
\end{align}
We can consider this a sparse regression with $\ell_1$ regularization problem, where design matrix $X$ is $(D^{(k+1)})^{-1}$. 

\subsection{Modification on $D$}
As we have seen, the trend filtering problem becomes a sparse regression with $\ell_1$ regularization if we consider the design matrix  $X = (D^{(k+1)})^{-1}$. However, $D^{(1)} \in \mathbb{R}^{(n-1)\times n}$ is not invertible, so is $D^{(k+1)}$ for $k=1,2,\dots$. By observation, we complete $D^{(1)}$ as a square and symmetric matrix  

\begin{align}
\hat{D}^{(1)} = \begin{bmatrix} 
    -1 & 1 & 0 & \dots & 0 & 0\\
    0 & -1 & 1 & \dots & 0 & 0\\
    \vdots & \ddots & \\
    0 & 0 & 0 & \dots & -1 & 1\\
    0 & 0 & 0 & \dots & 0 & -1
    \end{bmatrix}
    \in \mathbb{R}^{n\times n}.
\end{align}
And for $k\geq 1$, we obtain
\begin{align}
\hat{D}^{(k+1)} = \hat{D}^{(1)} \cdot \hat{D}^{(k)} \in \mathbb{R}^{n\times n}.
\end{align}
We notice that, by this modification, $\hat{D}^{(k+1)}$ has $k$ more rows added at the bottom without changing any previous entry. With this modification, we are able to invert $\hat{D}^{(k+1)}$ and consider the inverse matrix as an $X$ matrix in the sparse regression. 

\subsection{Special structure on $\hat{D}^{-1}$}
After determining the design matrix $X$ in the sparse regression problem, we could apply \susie algorithm to help us find a possible fit. Rather than generating $\hat{D}^{-1}$ and set this as an $X$ input, we exploit the special structure of $\hat{D}^{-1}$ and perform \susie on this specific trend filtering problem with $O(n)$ complexity. We will talk about how to make different computations linear in complexity by utilizing the special structure respectively. 

\subsection{Computation on $Xb$} \label{Computation on Xb}
In the trend filtering application, since $X = (\hat{D}^{(k+1)})^{-1}$, we obtain
\begin{align}
X \vb & = (\hat{D}^{(k+1)})^{-1} \vb = (\underbrace{\hat{D}^{(1)}\dots \hat{D}^{(1)}}_{k+1})^{-1} \vb \\
 & = \underbrace{(\hat{D}^{(1)})^{-1} \dots (\hat{D}^{(1)})^{-1}}_{k+1} \vb. 
\end{align}
We notice that since 
\begin{align}
(\hat{D}^{(1)})^{-1} = \begin{bmatrix} 
    -1 & -1 & -1 & \dots & -1 & -1\\
    0 & -1 & -1 & \dots & -1 & -1\\
    \vdots & \ddots & \\
    0 & 0 & 0 & \dots & -1 & -1\\
    0 & 0 & 0 & \dots & 0 & -1
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}
 
\begin{align}
(\hat{D}^{(1)})^{-1} \vb & = -1 \cdot [b_1+b_2+\dots+b_n, b_2+\dots+b_n, \dots, b_{n-1}+b_n, b_n]^T \\
& = -1 \cdot \text{cumsum}(\text{reverse}(\vb)) .
\end{align}
Let $f: \mathbb{R}^n \to \mathbb{R}^n$ such that $f(\vx)= -\text{cumsum}(\text{reverse}(\vx))$ for any $\vx \in \mathbb{R}^n$. Then
\begin{align}
X \vb & = (\hat{D}^{(k+1)})^{-1} \vb = f^{(k+1)}(\vb),
\end{align}
where $k$ is the order of trend filtering. 

\subsection{Computation on $X^T y$} \label{Computation on Xty}
We consider $X^T\vy$. Here $X = (\hat{D}^{(k+1)})^{-1}$ in the trend filtering problem, and $\vy$ is an n vector. We have

\begin{align}
X^T \vy & = ((\hat{D}^{(k+1)})^{-1})^T \vy = ((\underbrace{\hat{D}^{(1)}\dots \hat{D}^{(1)}}_{k+1})^{-1})^T \vy \\
& = \underbrace{((\hat{D}^{(1)})^{-1})^T \dots ((\hat{D}^{(1)})^{-1})^T}_{k+1} \vy.
\end{align}
Similarly, we observe that since
\begin{align}
((\hat{D}^{(1)})^{-1})^T = \begin{bmatrix} 
    -1 & 0 & 0 & \dots & 0 & 0\\
    -1 & -1 & 0 & \dots & 0 & 0\\
    \vdots & \ddots & \\
    -1 & -1 & -1 & \dots & -1 & 0\\
    -1 & -1 & -1 & \dots & -1 & -1
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}

\begin{align}
X^T \vy & = -1 \cdot [y_1, y_1+y_2, \dots, y_1+y_2+\dots+y_n]^T \\
& = -1 \cdot \text{cumsum}(\vy). 
\end{align}
Let $g: \mathbb{R}^n \to \mathbb{R}^n$ such that $g(\vx)= -\text{cumsum}(\vx)$ for any $\vx \in \mathbb{R}^n$. Then
\begin{align}
X^T \vy & = ((\hat{D}^{(k+1)})^{-1})^T \vy = g^{(k+1)}(\vy),
\end{align}
where $k$ is the order of trend filtering.

\subsection{Computation on $(X^2)^T 1$ (i.e. colSums($X^2$))} \label{Computation on d}

To compute $(X^2)^T \bm{1}$, where $X = (\hat{D}^{(k+1)})^{-1}$, let's first explore the special structure of $(\hat{D}^{(k+1)})^{-1}$ for $k=0,1,2$. 
\begin{align}
(\hat{D}^{(1)})^{-1} = \begin{bmatrix} 
    -1 & -1 & -1 & -1 & -1 & \dots \\
    0  & -1 & -1 & -1 & -1 & \dots \\
    0  & 0  & -1 & -1 & -1 & \dots \\
    0  & 0  & 0  & -1 & -1 & \dots \\
    \vdots & \ddots & \\
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}

\begin{align}
(\hat{D}^{(2)})^{-1} = \begin{bmatrix} 
    1  & 2 & 3 & 4 & 5 & 6 & \dots \\
    0  & 1 & 2 & 3 & 4 & 5 & \dots \\
    0  & 0 & 1 & 2 & 3 & 4 &\dots \\
    0  & 0 & 0 & 1 & 2 & 3 &\dots \\
    \vdots & \ddots & \\
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}

\begin{align}
(\hat{D}^{(3)})^{-1} = \begin{bmatrix} 
    -1 & -3 & -6 & -10 & -15 & \dots \\
    0  & -1 & -3 & -6  & -10 & \dots \\
    0  &  0 & -1 & -3  & -6  & \dots \\
    0  &  0 & 0  & -1  & -3  & \dots \\
    \vdots & \ddots & \\
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}

Define a triangular rotate matrix $X \in \mathbb{R}^{n\times n}$ such that 

(i) For any $i, j \leq n$, $X_{ij} = 0$ if $i>j$.

(ii) For any $k < n$, $X_{ab} = X_{cd}$ if $b-a = d-c = k$. 


We observe that if $X$ is a triangular rotate matrix, then 

\begin{align}
X^T \bm{1} = \text{cumsum}(X_{1.}).
\end{align}

Since $X^2$ is still a triangular rotate matrix, we obtain

\begin{align}
(X^2)^T \bm{1} = \text{cumsum}(X^2_{1.}).
\end{align}

Since $X = (\hat{D}^{(k+1)})^{-1}$ is a triangular rotate matrix, 

\begin{align}
(X^2)^T \bm{1} = \text{cumsum}(((\hat{D}^{(k+1)})^{-1})_{1.}^2).
\end{align}

And obviously, the first row of $(\hat{D}^{(k+1)})^{-1}$ is 
\begin{equation}
 ((\hat{D}^{(k+1)})^{-1})_{1.} = 
    \begin{cases} 
      \bm{-1} & \text{if } k = 0 \\
      g^{(k)}(\bm{1}) & \text{if } k > 0.
   \end{cases}
\end{equation}

\subsection{Computation on $\frac{1}{n}X^T 1$ (i.e. column means)} \label{Computation on cm}

By \ref{Computation on d}, we know that given that $X = (\hat{D}^{(k+1)})^{-1}$,
\begin{align}
 \frac{1}{n} X^T \bm{1} = \frac{1}{n} \text{cumsum}(X_{1.}) = \frac{1}{n} \text{cumsum}(((\hat{D}^{(k+1)})^{-1})_{1.}),
\end{align}
where $((\hat{D}^{(k+1)})^{-1})_{1.}$ is defined above in \ref{Computation on d}. 

\subsection{Computation on $s$ (i.e. column standard deviations)}
We consider for each column $j$, $j=1,2,\dots, n$,

\begin{align}
s_j & = \sqrt{E[X_{.j}^2] - E[X_{.j}]^2} \\
       & = \sqrt{\frac{1}{n}\sum_{i=1}^{n}X_{ij}^2 - (\frac{1}{n}\sum_{i=1}^{n} X_{ij})^2}.
\end{align}
Hence,

\begin{align}
\bm{s} & = \sqrt{E[X^2] - E[X]^2} \\
       & = \sqrt{\frac{1}{n}\text{colSums}(X^2) - (\frac{1}{n}\text{colSums}(X))^2} \\
       & = \sqrt{\frac{1}{n}(X^2)^T \bm{1} + (\frac{1}{n} X^T \bm{1})^2},
\end{align}
where the first term involves \ref{Computation on d} and the second term is computed in \ref{Computation on cm}.

\subsection{Computation on $(\hat{X}^2)^T 1$, where $\hat{X}$ is standardized} \label{Computation on std_d}

Given that $X = (\hat{D}^{(k+1)})^{-1}$. We first standardize $X$ to get $\hat{X}$. Our goal is to compute $(\hat{X}^2)^T \bm{1}$. By observation, 
\begin{equation}
(\hat{X}^2)^T \bm{1} = 
\begin{cases} 
      [\underbrace{n-1, n-1, \dots, n-1}_{n-1}, 0]^T & \text{if } k = 0 \\
      [\underbrace{n-1, n-1, \dots, n-1, n-1}_{n}]^T & \text{if } k \neq 0. 
\end{cases}
\end{equation}

\subsection{Conclusion}

Computation details from section \ref{Computation on Xb} to section \ref{Computation on std_d} explain how we can benefit from the unique structure of matrices from trend filtering problem. As shown by our formula, we do not need to form any matrix and complete \susie algorithm with $O(n)$ complexity. 
















