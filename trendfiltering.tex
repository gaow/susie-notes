    \section{\susie for trend filtering}
\subsection{Overview}

  Trend filtering is a useful statistical tool for nonparametric regression. \cite{Kim07l1trend} first proposed $\ell_1$ trend filtering for estimating underlying piecewise linear trends in time series data. This idea can be further extended to fit piecewise polynomial of degree $k$ to the data. In their paper, Kim et al. showed the equivalence between the $\ell_1$ trend filtering and the $\ell_1$-regularized least squares problem. This motivates us to think about the connection between trend filtering and sparse approximation in general. 

\subsection{Trend filtering and sparse regression}
Trend filtering problem is defined mathematically as follows. For a given integer $k \geq 0$, the kth order trend filtering is defined by a penalized least squares optimization problem,
\begin{align}
\hat{\vb} = \underset{\vb}{\mathrm{argmin}} \frac{1}{2}|| \vy- \vb||_2^2 + \frac{n^k}{k!}\lambda||D_{k+1}\vb||_1,
\end{align}
where $\vy = [y_1 \dots y_n]^T$ is an n vector of observations, $\lambda$ is a tuning parameter, and $D_{k+1}$ is the discrete difference operator of order $k$. When order $k=0$, $D$ is defined 

\begin{align}\label{D1}
D_{1} = \begin{bmatrix} 
    -1 & 1 & 0 & \dots & 0 & 0\\
    0 & -1 & 1 & \dots & 0 & 0\\
    \vdots & \ddots & \\
    0 & 0 & 0 & \dots & -1 & 1\\
    \end{bmatrix}
    \in \mathbb{R}^{(n-1)\times n}.
\end{align}
In this case, the components of the trend filtering estimate form a piecewise constant structure, with break points corresponding to the nonzero entries of $D_{1}\hat{\vb} = (\hat{b}_2 - \hat{b}_1, \dots, \hat{b}_n - \hat{b}_{n-1})$ \cite{Tibshirani2014}. And when $k\geq 1$, the operator $D_{k+1}$ is defined recursively,

\begin{align}\label{D1_2}
D_{k+1} = D_{1} \cdot D_{k} \in \mathbb{R}^{(n-k-1)\times n},
\end{align}
where the dot product is matrix multiplication. Notice that $D_1$ here in \ref{D1_2} is the $(n-k-1)\times (n-k)$ version of $D_1$ described in \ref{D1}.

Now we want to transform the trend filtering problem into a sparse regression problem. Let $\bm{\beta}=D_{k+1}\vb$. Then if $D_{k+1}$ were invertibe, we could write $\vb = (D_{k+1})^{-1}\bm{\beta}$ and the above problem would become

\begin{align}
\hat{\bm{\beta}} = \underset{\bm{\beta}}{\mathrm{argmin}} \frac{1}{2}||\vy- (D_{k+1})^{-1}\bm{\beta}||_2^2 + \frac{n^k}{k!}\lambda||\bm{\beta}||_1.
\end{align}
We can consider this a sparse regression with $\ell_1$ regularization problem, where the design matrix is $X_{k+1} = (D_{k+1})^{-1}$. 

\subsection{Modification on $D$}
As we have seen, the trend filtering problem becomes a sparse regression with $\ell_1$ regularization if we consider the design matrix  $X_{k+1} = (D_{k+1})^{-1}$. However, $D_{1} \in \mathbb{R}^{(n-1)\times n}$ is not invertible, so is $D_{k+1}$ for $k=1,2,\dots$. By observation, we complete $D_{1}$ as a square and symmetric matrix  

\begin{align}
\hat{D}_{1} = \begin{bmatrix} 
    -1 & 1 & 0 & \dots & 0 & 0\\
    0 & -1 & 1 & \dots & 0 & 0\\
    \vdots & \ddots & \\
    0 & 0 & 0 & \dots & -1 & 1\\
    0 & 0 & 0 & \dots & 0 & -1
    \end{bmatrix}
    \in \mathbb{R}^{n\times n}.
\end{align}
And for $k\geq 1$, we obtain
\begin{align}
\hat{D}_{k+1} = \hat{D}_{1} \cdot \hat{D}_{k} \in \mathbb{R}^{n\times n}.
\end{align}
We notice that, by this modification, $\hat{D}_{k+1}$ has $k$ more rows added at the bottom without changing any previous entry. With this modification, we are able to invert $\hat{D}_{k+1}$ and consider the inverse matrix as a design matrix in the sparse regression. 

\subsection{Special structure on $\hat{D}^{-1}$}
After determining the design matrix $X$ in the sparse regression problem, we could apply \susie algorithm to help us find a possible fit. Here, we denote $X_{k+1} = (\hat{D}_{k+1})^{-1}$, where $k$ is the order of trend filtering. Rather than generating $\hat{D}^{-1}$ and set this as an $X$ input, we exploit the special structure of $\hat{D}^{-1}$ and perform \susie on this specific trend filtering problem with $O(n)$ complexity. We will talk about how to make different computations linear in complexity by utilizing the special structure respectively. 

\subsection{Computation on $Xb$} \label{Computation on Xb}
In the trend filtering application, since $X_{k+1} = (\hat{D}_{k+1})^{-1}$, we obtain
\begin{align}
X_{k+1} \vb & = (\hat{D}_{k+1})^{-1} \vb = (\underbrace{\hat{D}_{1}\dots \hat{D}_{1}}_{k+1})^{-1} \vb \\
 & = \underbrace{(\hat{D}_{1})^{-1} \dots (\hat{D}_{1})^{-1}}_{k+1} \vb \\
 & = \underbrace{X_1 \dots X_1}_{k+1} \vb.
\end{align}
We notice that since 
\begin{align}
X_1 = \begin{bmatrix} 
    -1 & -1 & -1 & \dots & -1 & -1\\
    0 & -1 & -1 & \dots & -1 & -1\\
    \vdots & \ddots & \\
    0 & 0 & 0 & \dots & -1 & -1\\
    0 & 0 & 0 & \dots & 0 & -1
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}
 
\begin{align}
X_1 \vb & = -1 \cdot [b_1+b_2+\dots+b_n, b_2+\dots+b_n, \dots, b_{n-1}+b_n, b_n]^T \\
& = -1 \cdot \text{cumsum}(\text{reverse}(\vb)) .
\end{align}
Let $f: \mathbb{R}^n \to \mathbb{R}^n$ such that $f(\vx)= -\text{cumsum}(\text{reverse}(\vx))$ for any $\vx \in \mathbb{R}^n$. Then
\begin{align}
X_{k+1} \vb & = \underbrace{X_1 \dots X_1}_{k+1} \vb = f^{(k+1)}(\vb),
\end{align}
where $k$ is the order of trend filtering. 

\subsection{Computation on $X^T y$} \label{Computation on Xty}
We consider $X_{k+1}^T\vy$. Here $X_{k+1} = (\hat{D}_{k+1})^{-1}$ in the trend filtering problem, and $\vy$ is an n vector. We have

\begin{align}
X_{k+1}^T \vy & = ((\hat{D}_{k+1})^{-1})^T \vy = ((\underbrace{\hat{D}_{1}\dots \hat{D}_{1}}_{k+1})^{-1})^T \vy \\
& = \underbrace{((\hat{D}_{1})^{-1})^T \dots ((\hat{D}_{1})^{-1})^T}_{k+1} \vy \\
& = \underbrace{X_1^T \dots X_1^T}_{k+1} \vy.
\end{align}
Similarly, we observe that since
\begin{align}
X_1^T = \begin{bmatrix} 
    -1 & 0 & 0 & \dots & 0 & 0\\
    -1 & -1 & 0 & \dots & 0 & 0\\
    \vdots & \ddots & \\
    -1 & -1 & -1 & \dots & -1 & 0\\
    -1 & -1 & -1 & \dots & -1 & -1
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}

\begin{align}
 X_1^T \vy & = -1 \cdot [y_1, y_1+y_2, \dots, y_1+y_2+\dots+y_n]^T \\
& = -1 \cdot \text{cumsum}(\vy). 
\end{align}
Let $g: \mathbb{R}^n \to \mathbb{R}^n$ such that $g(\vx)= -\text{cumsum}(\vx)$ for any $\vx \in \mathbb{R}^n$. Then
\begin{align}
X_{k+1}^T \vy & = \underbrace{X_1^T \dots X_1^T}_{k+1} \vy = g^{(k+1)}(\vy),
\end{align}
where $k$ is the order of trend filtering.

\subsection{Computation on $(X^2)^T 1$ (i.e. colSums($X^2$))} \label{Computation on d}

To compute $(X_{k+1}^2)^T \bm{1}$, let's first explore the special structure of $X_{k+1}=(\hat{D}^{(k+1)})^{-1}$ for $k=0,1,2$. 
\begin{align}
X_1 = \begin{bmatrix} 
    -1 & -1 & -1 & -1 & -1 & \dots \\
    0  & -1 & -1 & -1 & -1 & \dots \\
    0  & 0  & -1 & -1 & -1 & \dots \\
    0  & 0  & 0  & -1 & -1 & \dots \\
    \vdots & \ddots & \\
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}

\begin{align}
X_2 = \begin{bmatrix} 
    1  & 2 & 3 & 4 & 5 & 6 & \dots \\
    0  & 1 & 2 & 3 & 4 & 5 & \dots \\
    0  & 0 & 1 & 2 & 3 & 4 &\dots \\
    0  & 0 & 0 & 1 & 2 & 3 &\dots \\
    \vdots & \ddots & \\
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}

\begin{align}
X_3 = \begin{bmatrix} 
    -1 & -3 & -6 & -10 & -15 & \dots \\
    0  & -1 & -3 & -6  & -10 & \dots \\
    0  &  0 & -1 & -3  & -6  & \dots \\
    0  &  0 & 0  & -1  & -3  & \dots \\
    \vdots & \ddots & \\
    \end{bmatrix}
    \in \mathbb{R}^{n\times n},
\end{align}

Define a triangular rotate matrix $Q \in \mathbb{R}^{n\times n}$ such that 

(i) For any $i, j \leq n$, $Q_{ij} = 0$ if $i>j$.

(ii) For any $k < n$, $Q_{ab} = Q_{cd}$ if $b-a = d-c = k$. 


We observe that if $X$ is a triangular rotate matrix, then 

\begin{align}
X^T \bm{1} = \text{cumsum}(X_{1.}).
\end{align}

Since $X^2$ is still a triangular rotate matrix, we obtain

\begin{align}
(X^2)^T \bm{1} = \text{cumsum}(X^2_{1.}).
\end{align}

Since $X_{k+1} = (\hat{D}_{k+1})^{-1}$ is a triangular rotate matrix, 

\begin{align}
(X_{k+1}^2)^T \bm{1} = \text{cumsum}((X_{k+1})_{1.}^2).
\end{align}

And obviously, the first row of $X_{k+1}$ is 
\begin{equation}
 (X_{k+1})_{1.} = 
    \begin{cases} 
      \bm{-1} & \text{if } k = 0 \\
      g^{(k)}(\bm{1}) & \text{if } k > 0.
   \end{cases}
\end{equation}

\subsection{Computation on $\mu$ (i.e. column means)} \label{Computation on cm}
For each column $j= 1,2,\dots,n,$
\begin{equation}
    \mu_j = E[X_{.j}],
\end{equation}
and $\bm{\mu} = [\mu_1, \mu_2, \dots, \mu_n]^T \in \mathbb{R}^n$. Hence we get 
\begin{equation}
 \bm{\mu} = \frac{1}{n} X_{k+1}^T \bm{1} = \frac{1}{n} \text{cumsum}((X_{k+1})_{1.}),
\end{equation}
where $(X_{k+1})_{1.}$ is defined above in \ref{Computation on d}. 

\subsection{Computation on $\sigma$ (i.e. column standard deviations)} \label{Computation on csd}
For each column $j=1,2,\dots, n$,

\begin{align}
\sigma_j & = \sqrt{E[X_{.j}^2] - E[X_{.j}]^2} \\
       & = \sqrt{\frac{1}{n}\sum_{i=1}^{n}X_{ij}^2 - (\frac{1}{n}\sum_{i=1}^{n} X_{ij})^2}.
\end{align}
Hence, $\bm{\sigma} = [\sigma_1, \sigma_2, \dots, \sigma_n]^T \in \mathbb{R}^n$ becomes

\begin{align}
\bm{\sigma} & = \sqrt{E[X^2] - E[X]^2} \\
       & = \sqrt{\frac{1}{n}\text{colSums}(X^2) - (\frac{1}{n}\text{colSums}(X))^2} \\
       & = \sqrt{\frac{1}{n}(X^2)^T \bm{1} + (\frac{1}{n} X^T \bm{1})^2},
\end{align}
where the first term involves \ref{Computation on d} and the second term is computed in \ref{Computation on cm}. Note that in the algorithm, we set the column standard deviation 1 when the column has variance 0 for computation convenience. 

\subsection{Computation on $(\hat{X}^2)^T 1$ (i.e. colSums($\hat{X}^2$))} \label{Computation on std_d}
We want to compute colSums($\hat{X}^2$), where $\hat{X}$ is scaled by both column means $\bm{\mu} = [\mu_1, \mu_2, \dots, \mu_n]^T \in \mathbb{R}^n$ and column standard deviations $\bm{\sigma} = [\sigma_1, \sigma_2, \dots, \sigma_n]^T \in \mathbb{R}^n$. We define $\hat{X} \in \mathbb{R}^{n\times n}$ such that for each $i=1,2,\dots,n$ and $j=1,2,\dots,n$, 
\begin{equation}
    \hat{X}_{ij} = \frac{X_{ij}-\mu_j}{\sigma_j}，
\end{equation}
where $X = X_{k+1} = (\hat{D}_{k+1})^{-1}$ if the order is $k$.  
We consider
\begin{align}
    \text{colSums}(\hat{X}^2) & = \sum_{i=1}^{n} \hat{X}_{i.}^2 \\
    & = [\sum_{i=1}^{n}\hat{X}_{i1}^2, \dots, \sum_{i=1}^{n}\hat{X}_{in}^2]^T \\
    & = [\sum_{i=1}^{n}(\frac{X_{i1}-\mu_1}{\sigma_1})^2, \dots, \sum_{i=1}^{n}(\frac{X_{in}-\mu_n}{\sigma_n})^2]^T  \\
    & = [\sum_{i=1}^{n}\frac{X_{i1}^2 - 2X_{i1}\mu_1 + \mu_1^2}{\sigma_1^2}, \dots, \sum_{i=1}^{n}\frac{X_{in}^2 - 2X_{in}\mu_n + \mu_n^2}{\sigma_n^2}]^T \\
    & = \big\{ \sum_{i=1}^{n} X_{i.}^2 - 2X^T \bm{1} \odot{(\mu_1, \dots, \mu_n)} + n(\mu_1, \dots, \mu_n)^2\big\} \oslash{(\sigma_1, \dots, \sigma_n)^2} \\
    & = (\text{colSums}(X^2) - 2X^T\bm{1}\odot{\bm{\mu}} + n\bm{\mu}^2)\oslash{\bm{\sigma}^2},
\end{align}
where colSums($X^2$) is computed by \ref{Computation on d}, $\odot$ is element-wise multiplication, $\oslash$ is element-wise division, $\bm{\mu}$ is an n vector of column means computed by \ref{Computation on cm}, and 
$\bm{\sigma}$ is an n vector of column standard deviations computed by \ref{Computation on csd}.
Interestingly, because of the special structure of $X_{k+1}$, we also observe that 

\begin{equation}
\text{colSums}(\hat{X}_{k+1}^2) = 
\begin{cases} 
      [\underbrace{n-1, n-1, \dots, n-1}_{n-1}, 0]^T & \text{if } k = 0 \\
      [\underbrace{n-1, n-1, \dots, n-1, n-1}_{n}]^T & \text{if } k \neq 0. 
\end{cases}
\end{equation}


\subsection{Conclusion}

Computation details from section \ref{Computation on Xb} to section \ref{Computation on std_d} explain how we can benefit from the unique structure of matrices from trend filtering problem. As shown by our formula, we do not need to form any matrix and complete \susie algorithm with $O(n)$ complexity. 
















